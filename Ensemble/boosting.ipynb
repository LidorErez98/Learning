{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This paper aims to provide an introduction to boosting algorithms. Boosting is a machine learning ensemble algorithms that aims to combine the predictions of several weak learners (models that are only slightly better than random guessing) to create a strong learner. The idea behind it, is to train a sequence of weak learners, where each one tries to correct the mistakes of its predecessor. This way, the overall model become more complex and thus, reduces the bias. The most popular boosting algorithms are AdaBoost, Gradient Boosting and XGBoost. In this paper, we will provide an overview of these algorithms and discuss their main characteristics. Note that I will use decision trees as weak learners in the examples, but it is possible to use other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The main idea in boosting is to combine the predictions of several weak learners to create a strong learner. The way it works is by training a sequence of weak learners (tipcally decision trees) where each one tries to correct the mistakes of its predecessor as mentioned before. Consider the following example: Lets say we want to classify between the two classes 1 and 0. We gather some data, conduct a research and decide we want to use boosting. Thus, we start by training a weak learner (a decision tree) on the data denoted as $f_1$ and get the following results:\n",
    "\n",
    "| Data Point | Prediction |\n",
    "|------------|------------|\n",
    "| 1          | 1          |\n",
    "| 0          | 1          |\n",
    "| 0          | 0          |\n",
    "| 1          | 0          |\n",
    "\n",
    "\n",
    "The weak learner $f_1$ has made some mistakes so to compensate for that, we train a second weak learner $f_2$ on the residuals of the first weak learner. Let $\\hat{y_i}$ be the prediction of the model for the i'th observation and $y_i$ be the true label. The residual for the i'th observation is defined as $y_i - \\hat{y_i}$ which is essentially the distance between the prediction and the true label. Back to our example, the residuals for the first weak learner are:\n",
    "\n",
    "| Data Point | Residual |\n",
    "|------------|----------|\n",
    "| 1          | 0        |\n",
    "| 0          | -1       |\n",
    "| 0          | 0        |\n",
    "| 1          | 1        |\n",
    "\n",
    "We now give a bigger weight to the observation that were misclassified by the first desicion tree. Then, we train a second decision tree. The second decision tree goal is to try and correct the mistakes of the first decision tree. This process can be repeated several times until we reach a stopping criteria (e.g. a maximum number of weak learners).\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "Decision trees are the most common weak learners used in boosting algorithms. A decision tree is a tree-like model of decisions and their possible consequences. The tree is composed of nodes and edges. The nodes represent the features in the dataset and the edges represent the decision rules. The tree is built by recursively splitting the data into subsets based on the features that give the most information gain (The split is determined by different criteria such as Gini impurity or entropy). As the tree grows, the model becomes more complex and can capture more patterns however, it is also prone to overfitting. In boosting, we use decision trees with a small depth (i.e. 3-10 levels) to prevent overfitting.\n",
    "\n",
    "![Decision Tree Diagram](./images/tree.png)\n",
    "\n",
    "*Figure 1. Decision Tree Diagram on the Iris Dataset created using sklearn. Source: [https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html)*\n",
    "\n",
    "\n",
    "\n",
    "## Objective Function \n",
    "\n",
    "\n",
    "Consider the objective function $$\\sum_{i=1}^{n} L(y_i, \\hat{y_i}) + \\sum_{k=1}^{K} \\Omega(f_k) + const$$ where $L$ is the loss function, $\\hat{y_i}$ is i'th the prediction of the model, $\\Omega(f_k)$ is the regularization term, $f_k$ is the k-th weak learner and $K$ is the number of weak learners. As the weak learners are mostly decision trees, we can't use algorithms such as gradient descent as trees are not represented as numerical vectors. Instead, we can use something called additive training. \n",
    "\n",
    "## Additive Training\n",
    "\n",
    "The idea behind additive training is to add a new weak learner to the model that minimizes the objective function. Let $\\hat{y_i}^{(0)}$ be the intial prediction for the i'th observation. We can use additive training to define the way we make predictions.\n",
    "\n",
    "The prediction for the i'th observation can be defined as:\n",
    "\n",
    "$$\\hat{y_i}^{(0)} = 0$$\n",
    "$$\\hat{y_i}^{(1)} = \\hat{y_i}^{(0)} + \\eta f_1(x_i)$$\n",
    "$$\\hat{y_i}^{(2)} = \\hat{y_i}^{(1)} + \\eta f_2(x_i)$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$\\hat{y_i}^{(K)} = \\hat{y_i}^{(k-1)} + \\eta f_K(x_i)$$\n",
    "\n",
    "Where $f_k(x_i)$ is the prediction of the k-th weak learner for the i'th observation,  $\\hat{y_i}^{(k-1)}$ are the predictions from previous rounds and $\\eta$ is the learning rate parameter which controls the contribution of each tree. So, applying additive training to our objective function, we get:\n",
    "\n",
    "$$\\sum_{i=1}^{n} L(y_i, \\hat{y_i}^{(k-1)} + \\eta f_K(x_i)) + \\sum_{k=1}^{K} \\Omega(f_k) + const$$\n",
    "\n",
    "So, the goal is to find the weak learner $f_k$ that minimizes the objective function. This can be done by using taylor expansion to approximate the objective function. \n",
    "\n",
    "## Taylor Expansion\n",
    "\n",
    "Recall the taylor expansion of a function $f(x)$ around a point $x_0$ is:\n",
    "\n",
    "$$f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x_0)(x-x_0)^n}{n!}$$\n",
    "\n",
    "However, we can ignore the higher order terms and approximate the function as:\n",
    "\n",
    "$$f(x) \\approx f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)(x-x_0)^2}{2!}$$\n",
    "\n",
    "We can apply the talyor expansion to the objective function since we're always around the point f_k(x_i). Thus, we get:\n",
    "\n",
    "$$obj^{(k)} \\approx \\sum_{i=1}^{n} [L(y_i, \\hat{y_i}^{k-1}) + g_i \\eta f_k(x_i) + \\frac{h_i \\eta^2 f_k^2(x_i)}{2!}] + \\Omega(f_k)$$\n",
    "\n",
    "where $g_i$ and $h_i$ are the first and second order derivatives of the loss function with respect to the prediction from the previous round $k-1$. The derivaties are defined as: $$g_i = \\frac{\\partial L(y_i, \\hat{y_i}^{k-1})}{\\partial \\hat{y_i}^{k-1}}$$  $$h_i = \\frac{\\partial^2 L(y_i, \\hat{y_i}^{k-1})}{\\partial^2 \\hat{y_i}^{k-1}}$$ \n",
    "\n",
    "\n",
    "### Why do we use the taylor expansion?\n",
    "\n",
    "In boosting, we aim to find the weak learner $f_k$ that minimizes the objective function, which includes the loss and regularization term (as defined above). However, directly minimizing this function is diffcult because decision trees are not differentiable. Taylor expansion allows us to approximate the objective function  in a way that makes it easier to optimize.\n",
    "\n",
    "## Regularization Term\n",
    "\n",
    "The regularization term $\\Omega(f_k)$ is used to prevent overfitting. The regularization term is defined as:\n",
    "\n",
    "$$\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2$$\n",
    "\n",
    "where $T$ is the number of leaves in the tree, $w_j$ is the prediction of the j'th leaf and $\\gamma$ is the complexity parameter that penalizes the number of leaves in the tree (deep trees are prone to overfitting) and $\\lambda$ is the regularization parameter that penalizes the complexity of the tree which is simply the L2 regularization term on the weights of the leaves.\n",
    "\n",
    "\n",
    "### How do we find gamma and lambda?\n",
    "\n",
    "The hyperparameters $\\gamma$ and $\\lambda$ are found using cross-validation. We train the model with different values of $\\gamma$ and $\\lambda$ and select the values that give the best performance on the validation set.\n",
    "\n",
    "\n",
    "# Boosting Process\n",
    "\n",
    "The boosting process can be summarized as follows:\n",
    "\n",
    "1. Intialize the prediction to a simple model, such as the mean (for regression) or the mode (for classification) of the target variable. This initial prediction is denoted as $\\hat{y_i}^{(0)}$.\n",
    "\n",
    "2. Iteratively Add Weak Learners:\n",
    "\n",
    "    * For each weak leaner $k$, perform the following steps:\n",
    "\n",
    "        a. **Create a Subset of the Data:** by randomly sampling (without replacement) using weighted sampling. \n",
    "\n",
    "        b. **Fit a Weak Learner:** $f_k$ to the subset of the data with the sample weights. \n",
    "\n",
    "        c. **Update the Predictions:** update the predictions using additive training:\n",
    "        $$\\hat{y_i}^{(k)} = \\hat{y_i}^{(k-1)} + \\eta f_k(x_i)$$\n",
    "\n",
    "        d. **Compute the Residuals:** $$r_i = y_i - \\hat{y_i}^{(k)}$$\n",
    "        \n",
    "        e. **Update the Sample Weights:** Can be done using various of heuristics such as: \n",
    "            * Using the residuals: \n",
    "            $$w_i = \\exp(-r_i)$$\n",
    "            * Using the learning rate: \n",
    "            $$w_i = w_i*(1+\\eta)$$. \n",
    "            \n",
    "3. Repeat step 2 until the number of weak learners reaches a stopping criterion:\n",
    "\n",
    "    * A maximum number of weak leaners (Iteration) $K$.\n",
    "    * The improvement in the model's perfroamce falls below a certain threshold.\n",
    "    * Cross-validation indicates no further improvement in the model's performance.\n",
    "\n",
    "**Note: the rule of cross-validation regarding the number of trees is to choose the right number of trees that gives the best performance but also doesn't overfit the data.** \n",
    "\n",
    "![Boosting Process Diagram](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XuD6oRrgVqtaSwH-cu6SA.png)\n",
    "\n",
    "*Figure 2. Boosting Process Diagram. Source: Soni, B. (n.d.). Understanding Boosting in Machine Learning: A Comprehensive Guide. Medium. Retrieved June 28, 2024, from [https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6](https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation\n",
    "\n",
    "On this part, I will use boosting to classify the digits dataset from sklearn (Multi-Class Classification). I will implement the boosting method using decision trees classifiers as my weak learners. Additionaly, I will update the weight of the observations that were misclassified by the previous weak learner using the formula:\n",
    "\n",
    "$${w_i * (1+\\eta)}$$\n",
    "\n",
    "where $w_i$ is the weight of the i'th observation and $\\eta$ is the learning rate. Note that the weights will be normalized so that they sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "dataset = datasets.load_digits()\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n,m = X.shape\n",
    "images = X.reshape(n, int(np.sqrt(m)), int(np.sqrt(m)))\n",
    "\n",
    "# Plot some images\n",
    "\n",
    "fig, ax = plt.subplots(2,5, figsize = (10,5))\n",
    "\n",
    "for i in range(5):\n",
    "    ax[0,i].imshow(images[i], cmap = 'gray')\n",
    "    ax[1,i].imshow(images[i+5], cmap = 'gray')\n",
    "    ax[0,i].axis('off')\n",
    "    ax[1,i].axis('off')\n",
    "    ax[0,i].set_title('Label: {}'.format(y[i]))\n",
    "    ax[1,i].set_title('Label: {}'.format(y[i]))\n",
    "\n",
    "plt.suptitle(\"First 10 Images of the Dataset\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boosters():\n",
    "    def __init__(self, learning_rate = 0.01, boosters = 100):\n",
    "        \"\"\"\n",
    "        Learning rate: The step size at which the model learns (eta)\n",
    "        Boosters: The number of trees in the ensemble (Will be used as the stopping criteria for the model)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosters = boosters\n",
    "        self.models = []\n",
    "        self.predictions = []\n",
    "        self.intial_predictions = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the weights\n",
    "        self.weights = np.ones(y.shape)/y.shape[0]\n",
    "        \n",
    "        # Determine the initial prediction (Most Common Class)\n",
    "        self.intial_predictions = stats.mode(y).mode[0]\n",
    "        self.f_k = np.full(y.shape, self.intial_predictions, dtype=float)\n",
    "        self.predictions.append(self.f_k)\n",
    "        self.residuals = y - self.f_k\n",
    "        rows_idx = np.arange(X.shape[0])\n",
    "\n",
    "        for k in range(self.boosters):\n",
    "            # Create a subset of the data (row wise) - sample 80% of the data for each tree\n",
    "            sample_idx =  np.random.choice(rows_idx, size = (X.shape[0]*0.8) , replace=False, p=self.weights)\n",
    "            X_sample = X[sample_idx]\n",
    "            y_sample = y[sample_idx]\n",
    "            sample_weights = self.weights[sample_idx]\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            f_k = DecisionTreeClassifier(max_depth = 4)\n",
    "            f_k.fit(X=X_sample, y=y_sample, sample_weight=sample_weights)\n",
    "            self.models.append(f_k)\n",
    "            # Make predictions\n",
    "            y_hat = f_k.predict(X) # we predict on the entire dataset to update the residuals\n",
    "\n",
    "            # Update the predictions\n",
    "            self.f_k += self.learning_rate * y_hat\n",
    "            self.predictions.append(self.f_k)\n",
    "            # Update the residuals\n",
    "            self.residuals = y - self.f_k\n",
    "\n",
    "            # Update the weights\n",
    "            incorrect_preds = (y!=y_hat).astype(int)\n",
    "            self.sample_weights[incorrect_preds] *= (1+self.learning_rate)\n",
    "            # Normalize the weights\n",
    "            self.sample_weights /= np.sum(self.sample_weights)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        final_preds = np.full(X.shape[0], self.intial_predictions)\n",
    "\n",
    "        for model in self.models:\n",
    "            final_preds += self.learning_rate * model.predict(X)\n",
    "\n",
    "        \n",
    "        self.preds = np.round(final_preds).astype(int)\n",
    "        return self.preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "\n",
    "\n",
    "### References \n",
    "\n",
    "Ieftimov, V. (n.d.). Classifier Decision Trees. Retrieved June 28, 2024, from [https://ieftimov.com/posts/classifier-decision-trees/](https://ieftimov.com/posts/classifier-decision-trees/)\n",
    "\n",
    "\n",
    "Soni, B. (n.d.). Understanding Boosting in Machine Learning: A Comprehensive Guide. Medium. Retrieved June 28, 2024, from [https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6](https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
