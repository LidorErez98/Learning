{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Random Forest (RF) is a popular ensemble learning method that can be used for both classification and regression tasks. RF uses multiple decision trees (weak learners) that are trained on different subsets of the training data and make predictions by averaging the predictions of the individual trees. This way the algorithm reduces overfitting (reduces variance) and improves the generalization of the model.\n",
    "\n",
    "The objective of this notebook is to introduce Random Forest and demonstrate how to implement it for different tasks using the scikit-learn library. We will cover the following topics:\n",
    "\n",
    "- Random Forest for Classification\n",
    "- Random Forest for Regression\n",
    "- Feature Importance\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Background\n",
    "\n",
    "The Random Forest algorithm can be mathematically formulated as follows:\n",
    "\n",
    "## Bootstraping\n",
    "\n",
    "Randomly select samples from the training data with replacement to create multiple subsets of the data.\n",
    "\n",
    "   Let $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ be the training data, where $x_i$ is the input feature vector and $y_i$ is the target label. We create $B$ bootstrap samples $D_b$ by randomly selecting $n$ samples with replacement from $D$ Where $|D_b| < |D|$ and  $B$ is the number of trees in the forest.\n",
    "\n",
    "## Random Feature Selection\n",
    "\n",
    "Randomly select a subset of features at each node of the decision tree to split the data.\n",
    "\n",
    "   Let $m$ be the number of features in the dataset. At each node of the decision tree, we randomly select $m'$ features, where $m' \\leq m$, to split the data. The value of $m'$ is a hyperparameter of the algorithm and is usually set to $\\sqrt{m}$ for classification tasks and $m/3$ for regression tasks.\n",
    "\n",
    "## Decision Tree Training\n",
    "\n",
    "Train a decision tree on each subset of the data using the random feature selection.\n",
    "\n",
    "For each bootstrap sample $D_b$, we train a decision tree $T_b$ using the selected features at each node.\n",
    "\n",
    "## Voting \n",
    "\n",
    "For regression tasks, the final prediction is the average of the predictions of all the individual trees:\n",
    "   \n",
    "   $$\\hat{y} = \\frac{1}{B} \\sum_{b=1}^{B} T_b(x)$$\n",
    "   \n",
    "For classification tasks, the final prediction is the majority vote of the predictions of all the individual trees:\n",
    "   \n",
    "   $$\\hat{y} = \\text{argmax}_k \\sum_{b=1}^{B} \\mathbb{1}(T_b(x) = k)$$\n",
    "\n",
    "\n",
    "## Variance Reduction\n",
    "\n",
    "The Random Forest variance formulation is given by:\n",
    "\n",
    "$$\\text{Var}(\\hat{y}) = \\rho \\sigma^2 + \\frac{1-\\rho}{B} \\sigma^2$$\n",
    "\n",
    "where $\\rho$ is the correlation between the trees and $\\sigma^2$ is the variance of the individual trees. Different trees in the forest aren't perfectly correlated ($\\rho <1$) which means we can trust the trees to provide different predictions. If we take a look on the formula, we can separate it into two parts. The first part $\\rho\\sigma^2$ is the correlation between the trees thus when trees are more correlated the term gets higher. The second part $\\frac{1-\\rho}{B}\\sigma^2$ represents the reduction in variance due to averaging/majority voting. The more trees we have, the lower the term gets. \n",
    "\n",
    "### Does it mean that we can reduce the variance to zero? \n",
    "\n",
    "At first glance, it might seem so, but it's not true. This is because the correlation between the trees will never be zero since they are trained on so-called different subsets of the data, which are sampled with replacement. This means that the trees will have some similarities in the data they are trained on and in the features they are using to make the splits.\n",
    "\n",
    "Intuitively, as B (number of trees) goes to infinity, the variance of the RF goes to $\\rho\\sigma^2$. This means that the RF can't reduce the variance to zero but it can reduce it significantly.\n",
    "\n",
    "It's good to mention that training a large number of trees can be computationally expensive and may not always lead to better performance.\n",
    "\n",
    "### Is it worth reducing the variance?\n",
    "\n",
    "In machine learning, there's always a trade-off between bias and variance, which means when we reduce one of them, the other tends to increase. In the case of Random Forests (RF), we reduce the variance, which leads to a slight increase in bias. The reduction in variance is typically much faster and more significant than the increase in bias.\n",
    "\n",
    "However, this does not mean that the algorithm cannot become too simple, resulting in high bias. It's essential to strike a balance where the reduction in variance is achieved without overly increasing the bias, which could lead to underfitting.\n",
    "\n",
    "## Out-of-Bag Error\n",
    "\n",
    "The out-of-bag (OOB) error is an estimate of the generalization error of the Random Forest model. It is calculated as the average error of each tree on the samples that were not included in the bootstrap sample used to train the tree. The OOB error can be used to evaluate the performance of the model without the need for a separate validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Considerations and Hyperparameter Tuning\n",
    "\n",
    "When using Random Forest, there are several hyperparameters that can be tuned to improve the performance of the model. Some of the most important hyperparameters are:\n",
    "\n",
    "- `n_estimators`: The number of trees in the forest [Usually between 100-1000].\n",
    "\n",
    "- `max_depth`: The maximum depth of the decision trees [Usually between 5-30].\n",
    "\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node [Usually between 2-10].\n",
    "\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node [Usually between 1-5].\n",
    "\n",
    "- `max_features`: The number of features to consider when looking for the best split.\n",
    "\n",
    "- `bootstrap`: Whether to use bootstrap samples when building trees.\n",
    "\n",
    "- `oob_score`: Whether to use out-of-bag samples to estimate the generalization error.\n",
    "\n",
    "These hyperparameters can be tuned using tuning techniques which are not covered by this notebook. However, I will demonstrate how does hyperparameters affect the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Stanford Online. (2020, April 17). Lecture 10 - Decision Trees and Ensemble Methods | Stanford CS229: Machine Learning (Autumn 2018) [Video]. YouTube. https://www.youtube.com/watch?v=wr9gUr-eWdA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
